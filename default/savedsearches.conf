[DB inspection]
action.email.inline = 1
action.email.reportServerEnabled = 0
action.summary_index = 1
alert.digest_mode = True
alert.suppress = 0
alert.track = 0
cron_schedule = 2 0 * * *
description = DB inspection
dispatch.latest_time = +Infinity
enableSched = 1
realtime_schedule = 0
search = | inputlookup monitored_indexes.csv| fields index | dedup index | map maxsearches=99 search=" | `db_inspect_collection($$index$$)`"

[Update monitored list from REST]
action.email.inline = 1
action.email.reportServerEnabled = 0
action.summary_index = 0
alert.digest_mode = True
alert.suppress = 0
alert.track = 0
cron_schedule = 59 23 * * *
description = Retrieve an updated list of local indexes from REST
disabled = false
enableSched = 1
realtime_schedule = 0
search = | rest /services/data/indexes splunk_server=local count=0 | search disabled=0 title!=_blocksignature title!=_thefishbucket | rename title AS index | fields index | outputlookup monitored_indexes.csv

[Find DB inspection records today]
cron_schedule = 0 0 * * *
dispatch.earliest_time = @d
dispatch.latest_time = now
description = Simple search to find records today, used for determining whether the DB inspection search has already run.
enableSched = 0
search = index=summary search_name="DB inspection"

#### PACKAGING_RULE delete below here for the TA. ####

[FB - Index Host Cache]
dispatch.earliest_time = @d
dispatch.latest_time = now
cron_schedule = 5 0,4,8,12,16,20 * * *
description = Cache for FireBrigade host / index pulldowns
enableSched = 1
search = index=summary search_name="DB inspection" | dedup orig_host, orig_index | sort orig_host, orig_index | table orig_host,orig_index | outputlookup fb_hostname_index_cache

[FB - Record Bucket Movement History]
action.summary_index = 1
cron_schedule = 22 0-20/4 * * *
description = Capture the bucket movement events from the _internal index and keep them in summary (usually for longer than 30 days)
disabled = true
dispatch.earliest_time = -4h@m
dispatch.latest_time = now
enableSched = 1
search = eventtype=bucket_movement | `_fb_figure_idx_name_from_home_or_cold_path` | `_fb_filter_bucket_movement_to_single_eventtype` | table _time, idx, bucket_id, bucket, orig_eventtype, to, message

[FB - Record Index Configuration History]
action.summary_index = 1
cron_schedule = 7 0-20/4 * * *
description = Capture the index parameter definitions at restart time (or whenever they are initialized) for historical purpose
disabled = true
dispatch.earliest_time = -4h@m
dispatch.latest_time = now
enableSched = 1
search = eventtype=fb_index_init | table _time, idx, frozenTimePeriodInSecs, coldToFrozenScript, coldToFrozenDir, warmToColdScript maxHotBucketSize, maxTotalDataSizeMB, maxHotSpanSecs, maxHotIdleSecs, maxHotBuckets, quarantinePastSecs, quarantineFutureSecs, homePath_maxDataSizeBytes, coldPath_maxDataSizeBytes, repFactor, isSlave 

[FB - Warning for Index not in Volume]
counttype = number of events
cron_schedule = 10 0 * * *
description = Warn when an index is not part of a volume, but shares the same path
disabled = 1
dispatch.earliest_time = -1d@d
dispatch.latest_time = @d
enableSched = 1
quantity = 0
relation = greater than
search = index=_internal source=*splunkd.log* component=IndexConfig idx inside config error | rex "volume=(?<volume>\w+)\s" | eval volume="volume:" . volume | rex "homePath='(?<homePath_expanded>.+?)'" | rex "coldPath='(?<coldPath_expanded>.+?)'" | eval state=case(isnotnull(homePath_expanded), "Home (Hot + Warm)", isnotnull(coldPath_expanded), "Cold", 1=1, "unknown") | eval path=coalesce(homePath_expanded, coldPath_expanded, "NULL?") | stats values(volume) AS volume, values(state) AS "Bucket State(s)", values(path) AS Directories by host, idx | rename idx AS Index

[FB - Find fast-moving buckets]
alert.track = 0
cron_schedule = 1 0 * * *
description = Look for buckets that were created and frozen in the same 24h period, before the nightly dbinspect ever saw it
disabled = 1
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
enableSched = 1
search = index=_internal source=*splunkd.log* component=databasePartitionPolicy creating | rename title AS idx | join type=left splunk_server, title [| rest /services/data/indexes | fields splunk_server, title, coldPath_expanded, homePath_expanded ] | eval bucket_parent=mvappend(bucket_parent, coldPath_expanded, homePath_expanded) | join type=left splunk_server, bucket_parent, bucket_id [ search index=_internal source=*splunkd.log* component=BucketMover "will attempt to freeze" | fields splunk_server, bucket_parent, bucket_id, candidate | eval FROZEN="true" ] | where FROZEN="true" | table splunk_server, candidate

[FB - Warnings for indexes at risk of missing retention targets]
alert.track = 0
cron_schedule = 0 1 * * *
disabled = 1
dispatch.earliest_time = @d
dispatch.latest_time = now
enableSched = 1
search = earliest=@d `_fb_summary_recs_host("*")` | stats sum(sizeOnDiskMB) AS totaldisk max(endEpoch) AS endEpoch, min(startEpoch) AS startEpoch by orig_host, index| join type=left orig_host, index [ | rest /services/data/indexes | fields splunk_server title, maxTotalDataSizeMB, frozenTimePeriodInSecs| rename splunk_server AS orig_host, title AS index ] | eval span_back=round(relative_time(now(), "@d+1h")) - startEpoch | eval span_pct=round(span_back / frozenTimePeriodInSecs * 100) | rename comment AS "1 - quar, 2 - warn, 3 - extreme" | eval timewarn=case(totaldisk > .9 * maxTotalDataSizeMB AND span_pct < 50, 3, totaldisk > .9 * maxTotalDataSizeMB AND span_pct < 90, 2, totaldisk > .9 * maxTotalDataSizeMB AND span_back > frozenTimePeriodInSecs, 3) | search timewarn=3 | eval span_back=tostring(span_back, "duration") | eval cutoff=now()-frozenTimePeriodInSecs | eval frozenTimePeriodInSecs=tostring(frozenTimePeriodInSecs, "duration") | eval disk_pct=round(totaldisk / maxTotalDataSizeMB * 100, 2) . "%" | eval timewarn=case(timewarn=3, "extreme", timewarn=2, "warn", timewarn=1, "quar", 1=1, null()) | convert ctime(cutoff), ctime(*Epoch) | table orig_host, index, totaldisk, maxTotalDataSizeMB, disk_pct, startEpoch, endEpoch, frozenTimePeriodInSecs, cutoff, span_back | rename orig_host AS Indexer, index AS Index, totaldisk AS "Disk Usage", endEpoch AS "Latest Event" startEpoch AS "Earliest Event", cutoff AS "Age Cutoff", disk_pct AS "% of Capacity" frozenTimePeriodInSecs AS "Retention Limit", maxTotalDataSizeMB AS "Disk Limit", span_back AS "Age of Oldest Event"
